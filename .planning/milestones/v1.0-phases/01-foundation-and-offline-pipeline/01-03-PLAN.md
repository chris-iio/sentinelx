---
phase: 01-foundation-and-offline-pipeline
plan: 03
type: tdd
wave: 2
depends_on:
  - "01-01"
files_modified:
  - app/pipeline/extractor.py
  - tests/test_extractor.py
  - tests/test_pipeline.py
autonomous: true
requirements:
  - EXTR-01
  - EXTR-02
  - EXTR-05

must_haves:
  truths:
    - "Extractor finds all IOC types in mixed free-form text including SIEM snippets and threat reports"
    - "iocextract and iocsearcher are both used and their results are merged without duplicates"
    - "Deduplication collapses same normalized value to one entry"
    - "Extractor works on defanged input without requiring pre-normalization"
  artifacts:
    - path: "app/pipeline/extractor.py"
      provides: "IOC extraction using iocextract + iocsearcher"
      exports: ["extract_iocs"]
    - path: "tests/test_extractor.py"
      provides: "Unit tests for extraction from mixed text"
      min_lines: 40
    - path: "tests/test_pipeline.py"
      provides: "End-to-end pipeline integration test: extract -> normalize -> classify -> dedup"
      min_lines: 30
  key_links:
    - from: "app/pipeline/extractor.py"
      to: "iocextract"
      via: "Library import for URL, IP, hash extraction"
      pattern: "import iocextract"
    - from: "app/pipeline/extractor.py"
      to: "iocsearcher"
      via: "Library import for CVE and supplementary extraction"
      pattern: "from iocsearcher"
    - from: "tests/test_pipeline.py"
      to: "app/pipeline/extractor.py"
      via: "Full pipeline test calling extract -> normalize -> classify -> dedup"
      pattern: "extract_iocs.*normalize.*classify"
---

<objective>
TDD implementation of IOC extractor using iocextract + iocsearcher, plus end-to-end pipeline integration test.

Purpose: The extractor is the entry point of the pipeline — it takes raw pasted text and finds all IOC candidates using two complementary libraries. The integration test proves the full pipeline (extract -> normalize -> classify -> deduplicate) works end-to-end with realistic input.

Output: Working extractor and proven end-to-end pipeline with 80%+ coverage.
</objective>

<execution_context>
@/home/chris/.claude/get-shit-done/workflows/execute-plan.md
@/home/chris/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-and-offline-pipeline/01-RESEARCH.md
@.planning/phases/01-foundation-and-offline-pipeline/01-01-SUMMARY.md
@.planning/phases/01-foundation-and-offline-pipeline/01-02-SUMMARY.md
</context>

<feature>
  <name>IOC Extractor + Pipeline Integration</name>
  <files>app/pipeline/extractor.py, tests/test_extractor.py, tests/test_pipeline.py</files>
  <behavior>
    **Extractor** — `extract_iocs(text: str) -> list[dict]`:
    Takes free-form text and returns a list of raw IOC candidate dicts `{'raw': str, 'type_hint': str}`.

    Uses two libraries:
    1. `iocextract` — for URLs (with refanging), IPv4 addresses, hashes (MD5, SHA1, SHA256), emails
    2. `iocsearcher` — for CVEs, and supplementary types that iocextract misses

    The function:
    - Calls `iocextract.extract_urls(text, refang=True)` for URLs
    - Calls `iocextract.extract_ipv4s(text, refang=True)` for IPv4s
    - Calls `iocextract.extract_ipv6s(text)` for IPv6s
    - Calls `iocextract.extract_hashes(text)` for all hash types
    - Creates `Searcher()` at module level (per iocsearcher docs — create once, reuse)
    - Calls `_searcher.search_data(text)` for CVEs and supplementary types
    - Returns combined list of all candidates

    Test cases:
    - Single IPv4: `"Check IP 192.168.1.1 for suspicious activity"` -> finds `192.168.1.1`
    - Defanged URL: `"Visit hxxp://evil[.]com/malware"` -> finds URL
    - SHA256 hash: `"Hash: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"` -> finds hash
    - CVE: `"Vulnerability CVE-2025-49596 is critical"` -> finds CVE
    - Mixed input: realistic SIEM alert snippet with multiple IOC types
    - Empty input: `""` -> returns empty list
    - No IOCs: `"Hello world, no indicators here"` -> returns empty list

    **Pipeline Integration** — `run_pipeline(text: str) -> list[IOC]`:
    A convenience function in `extractor.py` (or tested as a sequence in test_pipeline.py) that chains:
    1. `extract_iocs(text)` -> raw candidates
    2. For each candidate: `normalize(raw)` -> canonical form
    3. For each normalized: `classify(normalized, raw)` -> IOC or None
    4. Deduplicate by `(type, value)` tuple — same normalized value = one entry (EXTR-05)
    5. Return deduplicated list of IOC objects

    Integration test cases:
    - Mixed defanged text with duplicates:
      `"Alert: hxxp://evil[.]com and hxxp://evil[.]com again, IP 192[.]168[.]1[.]1"` -> 2 unique IOCs (URL + IPv4)
    - Realistic threat report snippet with IPv4, domain, hash, CVE -> all types extracted and classified
    - Duplicate hash appearing twice -> collapsed to one IOC
    - Text with no IOCs -> empty list returned
  </behavior>
  <implementation>
    After tests pass:
    - `app/pipeline/extractor.py`:
      - Module-level `_searcher = Searcher()` for iocsearcher (create once)
      - `extract_iocs()` calls both libraries and merges results
      - `run_pipeline()` chains extract -> normalize -> classify -> dedup
      - Dedup uses `dict` keyed on `(IOCType, normalized_value)` — first occurrence wins for `raw_match`
    - Pure functions, no Flask context, no HTTP calls
    - No persistent storage of input text (SEC-14)
  </implementation>
</feature>

<verification>
1. `pytest tests/test_extractor.py -v` — all tests pass
2. `pytest tests/test_pipeline.py -v` — all tests pass
3. `pytest tests/ --cov=app/pipeline --cov-report=term-missing` — 80%+ coverage on pipeline module
4. Integration test proves deduplication works (same IOC appearing twice -> one result)
5. Integration test proves all 8 IOC types can be extracted from realistic input
6. No HTTP calls made during any test (extraction is offline-only)
</verification>

<success_criteria>
- Extractor finds IOCs from mixed free-form text using both iocextract and iocsearcher
- Full pipeline (extract -> normalize -> classify -> dedup) works end-to-end
- Deduplication collapses identical normalized IOCs
- 80%+ test coverage on pipeline module
- All tests pass with `pytest -v`
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-and-offline-pipeline/01-03-SUMMARY.md`
</output>
